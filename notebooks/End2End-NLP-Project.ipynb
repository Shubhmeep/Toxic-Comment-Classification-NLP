{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f38e2a-9998-4c73-bfc3-21b74d64a5ee",
   "metadata": {},
   "source": [
    "### End 2 End NLP Project\n",
    "+ Emotion Detection In Text \n",
    "+ Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0814628-3d83-4fd6-a511-2eccf79f9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EDA Pkgs\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0d580d-c31c-44b7-b09b-10225857eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Viz Pkgs\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91eccfbf-d4d0-4e16-b0f7-2d7941efddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text Cleaning Pkgs\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e7e868-35fb-483f-82b6-842a29ef1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ML Pkgs\n",
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b209e004-ab77-4407-8689-b4318944d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/toxicsorted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea2d4c0-3bdd-405e-ab69-507ceaac36cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>Walking down the drive is the longest part of ...</td>\n",
       "      <td>8675854986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toxic</td>\n",
       "      <td>Suitecivil your a pussy ass bitch i'd fucken k...</td>\n",
       "      <td>2128620793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>A colleague asked me to study with her. I coul...</td>\n",
       "      <td>4482406478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>I was angry about how the government acted con...</td>\n",
       "      <td>4685690966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>Ah!! I LOVE man deodorant! I didn't know other...</td>\n",
       "      <td>1930976523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Emotion                                               Text      Random\n",
       "0     joy  Walking down the drive is the longest part of ...  8675854986\n",
       "1   toxic  Suitecivil your a pussy ass bitch i'd fucken k...  2128620793\n",
       "2   anger  A colleague asked me to study with her. I coul...  4482406478\n",
       "3   anger  I was angry about how the government acted con...  4685690966\n",
       "4     joy  Ah!! I LOVE man deodorant! I didn't know other...  1930976523"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430565a3-cf3b-4c6f-afa5-bafd084f5676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         10840\n",
       "sadness      6597\n",
       "fear         5296\n",
       "toxic        5203\n",
       "anger        4192\n",
       "surprise     3990\n",
       "neutral      2219\n",
       "disgust       841\n",
       "shame         144\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value Counts\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f87847-a91c-4bd6-a307-d746eb5aa9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Walking down the drive is the longest part of ...\n",
       "1        Suitecivil your a pussy ass bitch i'd fucken k...\n",
       "2        A colleague asked me to study with her. I coul...\n",
       "3        I was angry about how the government acted con...\n",
       "4        Ah!! I LOVE man deodorant! I didn't know other...\n",
       "                               ...                        \n",
       "39317      That's like making children talk bad about t...\n",
       "39318    Joy remember what I told you earlier about put...\n",
       "39319     Oh yes , I loved it . Wasn't the scene with t...\n",
       "39320    Only with   would I try to get a bottle of wat...\n",
       "39321    My christmas decorations in my room are making...\n",
       "Name: Clean_Text, Length: 39322, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User handles\n",
    "df['Clean_Text'] = df['Text'].apply(nfx.remove_userhandles)\n",
    "df['Clean_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03886bc3-1ac4-4f1b-842b-e5d2d770ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bfdd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heading natural state quality time'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing steps - remove numbers, capital letters, punctuation, '\\n'\n",
    "import re\n",
    "import string\n",
    "\n",
    "# remove all numbers with letters attached to them\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space\n",
    "# .lower() - convert all strings to lowercase \n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "# Remove all '\\n' in the string and replace it with a space\n",
    "remove_n = lambda x: re.sub(\"\\n\", \" \", x)\n",
    "\n",
    "# Remove all non-ascii characters \n",
    "remove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n",
    "\n",
    "# Apply all the lambda functions wrote previously through .map on the comments column\n",
    "df['Clean_Text'] = df['Clean_Text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)\n",
    "\n",
    "df['Clean_Text'][34844]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a0fcc0c-4adf-4f0b-b226-164659ad70ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Random</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>Walking down the drive is the longest part of ...</td>\n",
       "      <td>8675854986</td>\n",
       "      <td>walking drive longest journey school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toxic</td>\n",
       "      <td>Suitecivil your a pussy ass bitch i'd fucken k...</td>\n",
       "      <td>2128620793</td>\n",
       "      <td>suitecivil pussy ass bitch i d fucken kick ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>A colleague asked me to study with her. I coul...</td>\n",
       "      <td>4482406478</td>\n",
       "      <td>colleague asked study her  explain things perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>I was angry about how the government acted con...</td>\n",
       "      <td>4685690966</td>\n",
       "      <td>angry government acted concerning hainburg  es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>Ah!! I LOVE man deodorant! I didn't know other...</td>\n",
       "      <td>1930976523</td>\n",
       "      <td>ah   love man deodorant  know girls     rt man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39317</th>\n",
       "      <td>disgust</td>\n",
       "      <td>@OccupyWallStNYC That's like making children t...</td>\n",
       "      <td>3386560397</td>\n",
       "      <td>that s like making children talk bad parent di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39318</th>\n",
       "      <td>joy</td>\n",
       "      <td>Joy remember what I told you earlier about put...</td>\n",
       "      <td>5717224347</td>\n",
       "      <td>joy remember told earlier putting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39319</th>\n",
       "      <td>joy</td>\n",
       "      <td>Oh yes , I loved it . Wasn't the scene with t...</td>\n",
       "      <td>7703274024</td>\n",
       "      <td>oh yes   loved   scene judge great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39320</th>\n",
       "      <td>surprise</td>\n",
       "      <td>Only with @RoxanStuckey would I try to get a b...</td>\n",
       "      <td>8242834733</td>\n",
       "      <td>try bottle water realize tequila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39321</th>\n",
       "      <td>joy</td>\n",
       "      <td>My christmas decorations in my room are making...</td>\n",
       "      <td>7007774859</td>\n",
       "      <td>christmas decorations room making happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39322 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                               Text  \\\n",
       "0           joy  Walking down the drive is the longest part of ...   \n",
       "1         toxic  Suitecivil your a pussy ass bitch i'd fucken k...   \n",
       "2         anger  A colleague asked me to study with her. I coul...   \n",
       "3         anger  I was angry about how the government acted con...   \n",
       "4           joy  Ah!! I LOVE man deodorant! I didn't know other...   \n",
       "...         ...                                                ...   \n",
       "39317   disgust  @OccupyWallStNYC That's like making children t...   \n",
       "39318       joy  Joy remember what I told you earlier about put...   \n",
       "39319       joy   Oh yes , I loved it . Wasn't the scene with t...   \n",
       "39320  surprise  Only with @RoxanStuckey would I try to get a b...   \n",
       "39321       joy  My christmas decorations in my room are making...   \n",
       "\n",
       "           Random                                         Clean_Text  \n",
       "0      8675854986               walking drive longest journey school  \n",
       "1      2128620793  suitecivil pussy ass bitch i d fucken kick ass...  \n",
       "2      4482406478  colleague asked study her  explain things perf...  \n",
       "3      4685690966  angry government acted concerning hainburg  es...  \n",
       "4      1930976523  ah   love man deodorant  know girls     rt man...  \n",
       "...           ...                                                ...  \n",
       "39317  3386560397  that s like making children talk bad parent di...  \n",
       "39318  5717224347                  joy remember told earlier putting  \n",
       "39319  7703274024               oh yes   loved   scene judge great    \n",
       "39320  8242834733                   try bottle water realize tequila  \n",
       "39321  7007774859          christmas decorations room making happy    \n",
       "\n",
       "[39322 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "450c39c0-79dd-4eaf-85fe-57e344eb81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features & Labels\n",
    "Xfeatures = df['Clean_Text']\n",
    "ylabels = df['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d7f976-c28f-449e-ae1a-53a42bbda4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split Data\n",
    "x_train,x_test,y_train,y_test = train_test_split(Xfeatures,ylabels,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f086f29-dba9-40d2-a9dd-f06a6cca3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b81cc86-2bef-40c2-b9a3-668caaadaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression Pipeline\n",
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc64b9a7-efe2-4bc4-a0e7-46dff1d52b31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Fit Data\n",
    "pipe_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "135ed6f8-56ff-4d53-85e3-541e3a7ae2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28396371-5f5c-4a3b-b974-164e047764f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6486394846147325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Accuracy\n",
    "pipe_lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3a26b6-d09e-422f-991b-b08c48f55b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make A Prediction\n",
    "ex1 = \"stupid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b08597d9-6f59-45cb-a648-95b0da1ce313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['toxic'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.predict([ex1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3822ac-17fc-43dd-9bb7-8dad07a4d32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05672481, 0.01412131, 0.10722174, 0.3780715 , 0.05052071,\n",
       "        0.17130453, 0.00133299, 0.12692217, 0.09378023]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction Prob\n",
    "pipe_lr.predict_proba([ex1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7c4596-d643-48e5-a777-79a6f55c49da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'shame',\n",
       "       'surprise', 'toxic'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Know the classes\n",
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0d40f62-b1fd-4748-a279-c8f50c748f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Model & Pipeline\n",
    "# import joblib\n",
    "# pipeline_file = open(\"emotion_classifier_pipe_lr_03_june_2021.pkl\",\"wb\")\n",
    "# joblib.dump(pipe_lr,pipeline_file)\n",
    "# pipeline_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bfa146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(word, n=2):\n",
    "  ngram={}\n",
    "  total=26**n\n",
    "  for i in range(total):\n",
    "    c=''\n",
    "    k=i\n",
    "    for j in range(n):\n",
    "      c=chr(97+(k%26))+c\n",
    "      k=k//26\n",
    "      ngram[c]=set()\n",
    "  lexicon=words.words()\n",
    "  lexicon=[i.lower() for i in lexicon if i.isalnum()]\n",
    "  for w in range(len(lexicon)):\n",
    "    for c in range(0,len(lexicon[w])- n+1):\n",
    "      ngram[lexicon[w][c:c+n]].add(w)\n",
    "  freq_dict={}\n",
    "  for c in range(0, len(word)-n+1):\n",
    "    for w in ngram[word[c:c+n]]:\n",
    "      if lexicon[w] not in freq_dict.keys():\n",
    "        freq_dict[lexicon[w]]=1\n",
    "      else:\n",
    "        freq_dict[lexicon[w]]+=1\n",
    "  top_freq=dict(sorted(freq_dict.items(),key=operator.itemgetter(1),reverse=True)[:10])\n",
    "  return top_freq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc83f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fuck': {'bucket': 3, 'bucky': 2, 'chuck': 2, 'tuckahoe': 5, 'mucker': 3, 'luckful': 4, 'bucketful': 6, 'truckful': 5, 'unluckful': 6, 'neckful': 5}, 'wanker': {'dunker': 2, 'junker': 2, 'junkerdom': 5, 'junkerism': 5, 'hunker': 2, 'twanker': 1, 'swanker': 1, 'volkerwanderung': 10, 'awake': 3, 'wagnerite': 5}}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "from nltk.corpus import words\n",
    "import operator\n",
    "import nltk as nl\n",
    "\n",
    "cuss_word_array_uppercase = [\"Fuck\",\"Fuck you\",\"Ass\",\"Shit\",\"Piss off\",\"Dick head\",\"Asshole\",\"Son of a bitch\",\"Bastard\",\"Bitch\",\"Damn\",\"Cunt\",\"Bollocks\",\"Bugger\"\n",
    "                                                            ,\"Bloody Hell\",\"Choad\",\"Crikey\",\"Rubbish\",\"Shag\",\"Wanker\",\"Taking the piss\",\"Twat\",\"Bloody Oath\"\n",
    "                                                            , \"Arse\",\"Bloody\",\"Bugger\",\"Crap\",\"Damn\",\"Arsehole\",\"Balls\",\"Tits\",\"Boobs\",\"Cock\",\"Dick\",\"Pussy\",\"Cunt\",\"motherfuck\",\"fatherfuck\",\"Nigga\"]\n",
    "cuss_word_array_lowercase = []\n",
    "for word in cuss_word_array_uppercase:\n",
    "    cuss_word_array_lowercase.append(word.lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "comment = \"fucking wanker\"\n",
    "m={}\n",
    "\n",
    "\n",
    "if (pipe_lr.predict([comment])) == 'toxic':\n",
    "\n",
    "\n",
    "    #tokenization of words and removing punctuations in the sentence\n",
    "    x = string.punctuation\n",
    "    data = word_tokenize(comment)\n",
    "    clean_punct_words = [w for w in data if w not in x]\n",
    "    clean_words = [w for w in clean_punct_words if w not in stop]\n",
    "\n",
    "    stemmed = []\n",
    "    med = {}\n",
    "\n",
    "    for i in clean_words:\n",
    "        #apply snowball stemming\n",
    "        stemmed.append(snow_stemmer.stem(i))\n",
    "\n",
    "    #print(stemmed)\n",
    "   \n",
    "    for j in stemmed:\n",
    "        cuss = {}\n",
    "        if j in cuss_word_array_lowercase:\n",
    "            for dd in spell_check(j).keys():\n",
    "                cuss[dd] = nl.edit_distance(j, dd, transpositions=True)\n",
    "                \n",
    "                #cuss[dd] = editDistance(j, dd, len(j), len(dd))\n",
    "                #print(j,\"-> \", dd, \"\\n\",\"The minimum edit distance is:\", editDistance(j, dd, len(j), len(dd)))\n",
    "            #print(cuss)\n",
    "            med[j] = cuss\n",
    "    print(med)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "              \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04bb401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck bucky\n",
      "wanker twanker\n",
      "buckying twanker\n"
     ]
    }
   ],
   "source": [
    "for k in med.keys():\n",
    "    \n",
    "    x = min(med[k].values())\n",
    "    for i in med[k].keys():\n",
    "        if med[k][i] == x:\n",
    "            print(k,i)\n",
    "            comment = comment.replace(k,i)\n",
    "            break\n",
    "\n",
    "print(comment)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
